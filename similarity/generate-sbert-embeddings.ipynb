{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12171933,"sourceType":"datasetVersion","datasetId":7666016},{"sourceId":12171965,"sourceType":"datasetVersion","datasetId":7666030},{"sourceId":435832,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":355467,"modelId":376777}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!/usr/bin/env python3\n\"\"\"\nData Embeddings Generation Script\n\nThis script generates embeddings for bug and feature data using SBERT models with \nfour different preprocessing approaches:\n1. Bug data with filename: vocabulary filtering → stopwords/lemmatization → filename appending\n2. Bug data without filename: stopwords/lemmatization → vocabulary filtering\n3. Feature data with filename: vocabulary filtering → stopwords/lemmatization → filename appending  \n4. Feature data without filename: stopwords/lemmatization → vocabulary filtering\n\nThe script adds two columns to each input CSV:\n- 'with_filename_embeddings': Embeddings using with-filename preprocessing\n- 'without_filename_embeddings': Embeddings using without-filename preprocessing\n\nUsage:\n    python generate_embeddings.py \\\n        --bug_csv_path \"/path/to/bug/reference data.csv\" \\\n        --feature_csv_path \"/path/to/feature/reference data.csv\" \\\n        --bug_with_filename_model \"/path/to/bug/with filename/sbert bug.pt\" \\\n        --bug_without_filename_model \"/path/to/bug/without filename/sbert augmented bug.pt\" \\\n        --feature_with_filename_model \"/path/to/feature/with filename/sbert augmented feature.pt\" \\\n        --feature_without_filename_model \"/path/to/feature/without filename/sbert augmented features.pt\" \\\n        --bug_with_filename_vocab \"/path/to/bug/with filename/vocabulary.csv\" \\\n        --bug_without_filename_vocab \"/path/to/bug/without filename/vocabulary.csv\" \\\n        --feature_with_filename_vocab \"/path/to/feature/with filename/vocabulary.csv\" \\\n        --feature_without_filename_vocab \"/path/to/feature/without filename/vocabulary.csv\"\n\"\"\"\n\nimport argparse\nimport os\nimport sys\nimport re\nimport json\nimport time\nfrom pathlib import Path\nfrom typing import List, Dict, Tuple, Optional, Set\n\nimport torch\nimport numpy as np\nimport pandas as pd\nimport spacy\nfrom spacy.cli import download\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\n\n# Ensure reproducibility\nnp.random.seed(42)\ntorch.manual_seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(42)\n    torch.cuda.manual_seed_all(42)\n\nclass EmbeddingGenerator:\n    \"\"\"\n    Generates embeddings for text data using SBERT models with different preprocessing approaches.\n    Mirrors the exact preprocessing logic from the classification system.\n    \"\"\"\n    \n    def __init__(self, device: str = 'auto'):\n        \"\"\"\n        Initialize the embedding generator.\n        \n        Args:\n            device: Device to use ('auto', 'cuda', 'cpu')\n        \"\"\"\n        # Set device\n        if device == 'auto':\n            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        else:\n            self.device = torch.device(device)\n            \n        print(f\"Using device: {self.device}\")\n        \n        # Initialize spaCy\n        self._init_nlp()\n        \n        # Storage for models and vocabularies\n        self.models = {}\n        self.vocabularies = {}\n        \n        print(\"EmbeddingGenerator initialized successfully\")\n    \n    def _init_nlp(self):\n        \"\"\"Initialize spaCy model for text preprocessing.\"\"\"\n        try:\n            self.nlp = spacy.load('en_core_web_sm')\n            print(\"Loaded spaCy model 'en_core_web_sm'\")\n        except OSError:\n            print(\"Downloading spaCy model 'en_core_web_sm'...\")\n            download('en_core_web_sm')\n            self.nlp = spacy.load('en_core_web_sm')\n            print(\"Downloaded and loaded spaCy model 'en_core_web_sm'\")\n    \n    def load_vocabulary(self, vocab_path: str, key: str):\n        \"\"\"\n        Load vocabulary from CSV file.\n        \n        Args:\n            vocab_path: Path to vocabulary CSV file\n            key: Key to store vocabulary under\n        \"\"\"\n        print(f\"Loading vocabulary from {vocab_path}\")\n        \n        if not os.path.exists(vocab_path):\n            raise FileNotFoundError(f\"Vocabulary file not found: {vocab_path}\")\n        \n        try:\n            vocab_df = pd.read_csv(vocab_path)\n            # Assuming vocabulary is in the first column\n            vocab_words = set(vocab_df.iloc[:, 0].astype(str).str.lower().tolist())\n            self.vocabularies[key] = vocab_words\n            print(f\"Loaded {len(vocab_words)} vocabulary words for {key}\")\n        except Exception as e:\n            raise RuntimeError(f\"Error loading vocabulary from {vocab_path}: {str(e)}\")\n    \n    def load_sbert_model(self, model_path: str, key: str):\n        \"\"\"\n        Load SBERT model from file.\n        \n        Args:\n            model_path: Path to SBERT model file\n            key: Key to store model under\n        \"\"\"\n        print(f\"Loading SBERT model from {model_path}\")\n        \n        if not os.path.exists(model_path):\n            raise FileNotFoundError(f\"Model file not found: {model_path}\")\n        \n        try:\n            # Load the model using SentenceTransformer with all-mpnet-base-v2 architecture\n            model = SentenceTransformer('all-mpnet-base-v2')\n            \n            # Load the fine-tuned weights\n            state_dict = torch.load(model_path, map_location=self.device)\n            \n            # Filter out classifier layers that don't belong to SentenceTransformer\n            # Keep only the layers that are part of the sentence transformer\n            filtered_state_dict = {}\n            for key_name, value in state_dict.items():\n                # Skip classifier layers\n                if not key_name.startswith('classifier.'):\n                    filtered_state_dict[key_name] = value\n            \n            # Load the filtered state dict with strict=False to allow missing keys\n            model.load_state_dict(filtered_state_dict, strict=False)\n            \n            # Move to device and set to eval mode\n            model = model.to(self.device)\n            model.eval()\n            \n            self.models[key] = model\n            print(f\"Loaded SBERT model for {key}\")\n            \n        except Exception as e:\n            raise RuntimeError(f\"Error loading SBERT model from {model_path}: {str(e)}\")\n    \n    def _clean_text(self, text: str) -> str:\n        \"\"\"\n        Basic text cleaning (steps 1-4 from classification preprocessing).\n        \n        Args:\n            text: Input text to clean\n            \n        Returns:\n            Cleaned text\n        \"\"\"\n        if pd.isna(text) or text is None:\n            return \"\"\n        \n        text = str(text)\n        \n        # Convert to lowercase\n        text = text.lower()\n        \n        # Remove line breaks\n        text = text.replace('\\r', ' ')\n        text = text.replace('\\n', ' ')\n        \n        # Remove non-alphanumeric characters (keeping spaces)\n        text = re.sub(r'[^a-zA-Z0-9 ]', '', text)\n        \n        # Remove extra spaces\n        text = re.sub(r'\\s+', ' ', text).strip()\n        \n        return text\n    \n    def _preprocess_without_filename(self, title: str, description: str, comments: str, vocabulary_words: Set[str]) -> str:\n        \"\"\"\n        Preprocess text for \"without filename\" variant.\n        Uses title + description + comments (since no filename available)\n        Order: stopwords/lemmatization → vocabulary filtering\n        \n        Args:\n            title: Issue title\n            description: Issue description  \n            comments: Issue comments\n            vocabulary_words: Set of vocabulary words for filtering\n            \n        Returns:\n            Preprocessed text\n        \"\"\"\n        # Convert to strings and handle NaN values\n        title = str(title) if pd.notna(title) else \"\"\n        description = str(description) if pd.notna(description) else \"\"\n        comments = str(comments) if pd.notna(comments) else \"\"\n        \n        # 1-4. Basic cleaning - use title + description + comments (no filename available)\n        all_text = self._clean_text(title + \" \" + description + \" \" + comments)\n        \n        if not all_text:\n            return \"\"\n        \n        # 5. Remove stopwords and lemmatize FIRST\n        doc = self.nlp(all_text)\n        all_text = ' '.join([word.lemma_ for word in doc if not word.is_stop and word.lemma_.strip()])\n        \n        # 6. Filter words based on vocabulary\n        words = all_text.split()\n        filtered_words = [word for word in words if word in vocabulary_words]\n        all_text = ' '.join(filtered_words)\n        \n        return all_text\n    \n    def _preprocess_with_filename(self, title: str, description: str, comments: str, filename: str, vocabulary_words: Set[str]) -> str:\n        \"\"\"\n        Preprocess text for \"with filename\" variant.\n        Uses title + description (excludes comments when filename is available)\n        Order: vocabulary filtering → stopwords/lemmatization → filename appending\n        \n        Args:\n            title: Issue title\n            description: Issue description\n            comments: Issue comments (ignored when filename is present)\n            filename: Filename to append\n            vocabulary_words: Set of vocabulary words for filtering\n            \n        Returns:\n            Preprocessed text\n        \"\"\"\n        # Convert to strings and handle NaN values\n        title = str(title) if pd.notna(title) else \"\"\n        description = str(description) if pd.notna(description) else \"\"\n        comments = str(comments) if pd.notna(comments) else \"\"\n        filename = str(filename) if pd.notna(filename) else \"\"\n        \n        # 1-4. Basic cleaning - use title + description (exclude comments when filename present)\n        all_text = self._clean_text(title + \" \" + description)\n        \n        if not all_text:\n            all_text = \"\"\n        \n        # 5. Filter words based on vocabulary FIRST\n        words = all_text.split()\n        filtered_words = [word for word in words if word in vocabulary_words]\n        all_text = ' '.join(filtered_words)\n        \n        # 6. Remove stopwords and lemmatize\n        if all_text:\n            doc = self.nlp(all_text)\n            all_text = ' '.join([word.lemma_ for word in doc if not word.is_stop and word.lemma_.strip()])\n        \n        # 7. Add filename to the end\n        if filename and filename.strip() and filename.lower() != 'nan':\n            cleaned_filename = self._clean_text(filename)\n            if cleaned_filename:\n                all_text += \" \" + cleaned_filename\n        \n        return all_text\n    \n    def generate_embeddings_batch(self, texts: List[str], model_key: str, batch_size: int = 32) -> List[List[float]]:\n        \"\"\"\n        Generate embeddings for a batch of texts.\n        \n        Args:\n            texts: List of preprocessed texts\n            model_key: Key of the model to use\n            batch_size: Batch size for processing\n            \n        Returns:\n            List of embedding vectors\n        \"\"\"\n        if model_key not in self.models:\n            raise ValueError(f\"Model {model_key} not loaded\")\n        \n        model = self.models[model_key]\n        all_embeddings = []\n        \n        # Process in batches\n        for i in tqdm(range(0, len(texts), batch_size), desc=f\"Generating embeddings ({model_key})\"):\n            batch_texts = texts[i:i + batch_size]\n            \n            # Handle empty texts\n            batch_texts = [text if text else \" \" for text in batch_texts]\n            \n            with torch.no_grad():\n                batch_embeddings = model.encode(\n                    batch_texts,\n                    convert_to_tensor=True,\n                    device=self.device,\n                    show_progress_bar=False\n                )\n                \n                # Convert to CPU and numpy\n                batch_embeddings = batch_embeddings.cpu().numpy()\n                all_embeddings.extend(batch_embeddings.tolist())\n        \n        return all_embeddings\n    \n    def process_dataset(self, csv_path: str, request_type: str, batch_size: int = 32) -> pd.DataFrame:\n        \"\"\"\n        Process a dataset (bug or feature) and add embedding columns.\n        \n        Args:\n            csv_path: Path to the CSV file\n            request_type: 'bug' or 'feature'\n            batch_size: Batch size for embedding generation\n            \n        Returns:\n            DataFrame with added embedding columns\n        \"\"\"\n        print(f\"\\nProcessing {request_type} dataset: {csv_path}\")\n        \n        # Load data\n        if not os.path.exists(csv_path):\n            raise FileNotFoundError(f\"CSV file not found: {csv_path}\")\n        \n        df = pd.read_csv(csv_path)\n        print(f\"Loaded {len(df)} rows\")\n        \n        # Get model and vocabulary keys\n        with_filename_model_key = f\"{request_type}_with_filename\"\n        without_filename_model_key = f\"{request_type}_without_filename\"\n        with_filename_vocab_key = f\"{request_type}_with_filename_vocab\"\n        without_filename_vocab_key = f\"{request_type}_without_filename_vocab\"\n        \n        # Check if models and vocabularies are loaded\n        for key in [with_filename_model_key, without_filename_model_key]:\n            if key not in self.models:\n                raise ValueError(f\"Model {key} not loaded\")\n        \n        for key in [with_filename_vocab_key, without_filename_vocab_key]:\n            if key not in self.vocabularies:\n                raise ValueError(f\"Vocabulary {key} not loaded\")\n        \n        # Preprocess texts for both variants\n        print(\"Preprocessing texts...\")\n        \n        # Without filename preprocessing\n        without_filename_texts = []\n        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Preprocessing (without filename)\"):\n            text = self._preprocess_without_filename(\n                row.get('title', ''),\n                row.get('body', ''),\n                row.get('all_comments', ''),\n                self.vocabularies[without_filename_vocab_key]\n            )\n            without_filename_texts.append(text)\n        \n        # With filename preprocessing\n        with_filename_texts = []\n        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Preprocessing (with filename)\"):\n            text = self._preprocess_with_filename(\n                row.get('title', ''),\n                row.get('body', ''),\n                row.get('all_comments', ''),\n                row.get('filename', ''),\n                self.vocabularies[with_filename_vocab_key]\n            )\n            with_filename_texts.append(text)\n        \n        # Generate embeddings\n        print(\"Generating embeddings...\")\n        \n        without_filename_embeddings = self.generate_embeddings_batch(\n            without_filename_texts, \n            without_filename_model_key, \n            batch_size\n        )\n        \n        with_filename_embeddings = self.generate_embeddings_batch(\n            with_filename_texts, \n            with_filename_model_key, \n            batch_size\n        )\n        \n        # Add embedding columns to dataframe\n        print(\"Adding embedding columns to dataframe...\")\n        df['without_filename_embeddings'] = [str(emb) for emb in without_filename_embeddings]\n        df['with_filename_embeddings'] = [str(emb) for emb in with_filename_embeddings]\n        \n        print(f\"Successfully processed {request_type} dataset\")\n        return df\n\n\ndef main(args):\n    \"\"\"Main function to run the embedding generation script.\"\"\"\n    \n    print(\"=\" * 80)\n    print(\"DATA EMBEDDINGS GENERATION\")\n    print(\"=\" * 80)\n    print(f\"Device: {args.device}\")\n    print(f\"Batch size: {args.batch_size}\")\n    print()\n    \n    try:\n        # Initialize generator\n        generator = EmbeddingGenerator(device=args.device)\n        \n        # Load vocabularies\n        print(\"Loading vocabularies...\")\n        generator.load_vocabulary(args.bug_with_filename_vocab, 'bug_with_filename_vocab')\n        generator.load_vocabulary(args.bug_without_filename_vocab, 'bug_without_filename_vocab')\n        generator.load_vocabulary(args.feature_with_filename_vocab, 'feature_with_filename_vocab')\n        generator.load_vocabulary(args.feature_without_filename_vocab, 'feature_without_filename_vocab')\n        \n        # Load models\n        print(\"\\nLoading SBERT models...\")\n        generator.load_sbert_model(args.bug_with_filename_model, 'bug_with_filename')\n        generator.load_sbert_model(args.bug_without_filename_model, 'bug_without_filename')\n        generator.load_sbert_model(args.feature_with_filename_model, 'feature_with_filename')\n        generator.load_sbert_model(args.feature_without_filename_model, 'feature_without_filename')\n        \n        # Process bug dataset\n        print(\"\\n\" + \"=\" * 50)\n        print(\"PROCESSING BUG DATASET\")\n        print(\"=\" * 50)\n        start_time = time.time()\n        \n        bug_df = generator.process_dataset(args.bug_csv_path, 'bug', args.batch_size)\n        \n        # Save bug results\n        # Extract filename and save to working directory (Kaggle writable)\n        bug_filename = os.path.basename(args.bug_csv_path).replace('.csv', f'{args.output_suffix}.csv')\n        bug_output_path = os.path.join('/kaggle/working', bug_filename)\n        bug_df.to_csv(bug_output_path, index=False)\n        print(f\"Bug dataset saved to: {bug_output_path}\")\n        \n        bug_time = time.time() - start_time\n        print(f\"Bug dataset processing time: {bug_time:.2f} seconds\")\n        \n        # Process feature dataset\n        print(\"\\n\" + \"=\" * 50)\n        print(\"PROCESSING FEATURE DATASET\")\n        print(\"=\" * 50)\n        start_time = time.time()\n        \n        feature_df = generator.process_dataset(args.feature_csv_path, 'feature', args.batch_size)\n        \n        # Save feature results\n        # Extract filename and save to working directory (Kaggle writable)\n        feature_filename = os.path.basename(args.feature_csv_path).replace('.csv', f'{args.output_suffix}.csv')\n        feature_output_path = os.path.join('/kaggle/working', feature_filename)\n        feature_df.to_csv(feature_output_path, index=False)\n        print(f\"Feature dataset saved to: {feature_output_path}\")\n        \n        feature_time = time.time() - start_time\n        print(f\"Feature dataset processing time: {feature_time:.2f} seconds\")\n        \n        # Summary\n        print(\"\\n\" + \"=\" * 50)\n        print(\"PROCESSING COMPLETE\")\n        print(\"=\" * 50)\n        print(f\"Bug dataset: {len(bug_df)} rows processed → {bug_output_path}\")\n        print(f\"Feature dataset: {len(feature_df)} rows processed → {feature_output_path}\")\n        print(f\"Total processing time: {bug_time + feature_time:.2f} seconds\")\n        print(\"\\nEmbedding columns added:\")\n        print(\"- 'without_filename_embeddings': Embeddings using without-filename preprocessing\")\n        print(\"- 'with_filename_embeddings': Embeddings using with-filename preprocessing\")\n        \n    except Exception as e:\n        print(f\"\\nError: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n        sys.exit(1)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T10:27:43.778668Z","iopub.execute_input":"2025-06-15T10:27:43.779007Z","iopub.status.idle":"2025-06-15T10:27:43.814405Z","shell.execute_reply.started":"2025-06-15T10:27:43.778985Z","shell.execute_reply":"2025-06-15T10:27:43.813750Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"Generate embeddings for bug and feature data using SBERT models\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=__doc__\n    )\n    \n    # CSV file paths\n    parser.add_argument('--bug_csv_path', default='/kaggle/input/bug-and-feature-reference-data/bug reference data.csv', help='Path to bug reference data CSV')\n    parser.add_argument('--feature_csv_path', default='/kaggle/input/bug-and-feature-reference-data/feature reference data.csv', help='Path to feature reference data CSV')\n    \n    # Model paths\n    parser.add_argument('--bug_with_filename_model', default='/kaggle/input/sbert-similarity/pytorch/default/1/sbert bug with filename.pt', help='Path to bug with filename SBERT model')\n    parser.add_argument('--bug_without_filename_model', default='/kaggle/input/sbert-similarity/pytorch/default/1/sbert augmented bug without filename.pt', help='Path to bug without filename SBERT model')\n    parser.add_argument('--feature_with_filename_model', default='/kaggle/input/sbert-similarity/pytorch/default/1/sbert augmented feature with filename.pt', help='Path to feature with filename SBERT model')\n    parser.add_argument('--feature_without_filename_model', default='/kaggle/input/sbert-similarity/pytorch/default/1/sbert augmented features without filename.pt', help='Path to feature without filename SBERT model')\n    \n    # Vocabulary paths\n    parser.add_argument('--bug_with_filename_vocab', default='/kaggle/input/bug-and-features-word-filtering-vocabularies/bug with filename vocabulary.csv', help='Path to bug with filename vocabulary CSV')\n    parser.add_argument('--bug_without_filename_vocab', default='/kaggle/input/bug-and-features-word-filtering-vocabularies/bug without filename vocabulary.csv', help='Path to bug without filename vocabulary CSV')\n    parser.add_argument('--feature_with_filename_vocab', default='/kaggle/input/bug-and-features-word-filtering-vocabularies/feature with filename vocabulary.csv', help='Path to feature with filename vocabulary CSV')\n    parser.add_argument('--feature_without_filename_vocab', default='/kaggle/input/bug-and-features-word-filtering-vocabularies/feature without filename vocabulary.csv', help='Path to feature without filename vocabulary CSV')\n    \n    # Optional parameters\n    parser.add_argument('--batch_size', type=int, default=32, help='Batch size for embedding generation (default: 32)')\n    parser.add_argument('--device', choices=['auto', 'cuda', 'cpu'], default='auto', help='Device to use (default: auto)')\n    parser.add_argument('--output_suffix', default='_with_embeddings', help='Suffix for output files (default: _with_embeddings)')\n    \n    args, unknown = parser.parse_known_args()\n    main(args)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T10:27:43.891962Z","iopub.execute_input":"2025-06-15T10:27:43.892569Z","iopub.status.idle":"2025-06-15T10:30:37.718993Z","shell.execute_reply.started":"2025-06-15T10:27:43.892544Z","shell.execute_reply":"2025-06-15T10:30:37.718394Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nDATA EMBEDDINGS GENERATION\n================================================================================\nDevice: auto\nBatch size: 32\n\nUsing device: cuda\nLoaded spaCy model 'en_core_web_sm'\nEmbeddingGenerator initialized successfully\nLoading vocabularies...\nLoading vocabulary from /kaggle/input/bug-and-features-word-filtering-vocabularies/bug with filename vocabulary.csv\nLoaded 28194 vocabulary words for bug_with_filename_vocab\nLoading vocabulary from /kaggle/input/bug-and-features-word-filtering-vocabularies/bug without filename vocabulary.csv\nLoaded 45595 vocabulary words for bug_without_filename_vocab\nLoading vocabulary from /kaggle/input/bug-and-features-word-filtering-vocabularies/feature with filename vocabulary.csv\nLoaded 10083 vocabulary words for feature_with_filename_vocab\nLoading vocabulary from /kaggle/input/bug-and-features-word-filtering-vocabularies/feature without filename vocabulary.csv\nLoaded 17498 vocabulary words for feature_without_filename_vocab\n\nLoading SBERT models...\nLoading SBERT model from /kaggle/input/sbert-similarity/pytorch/default/1/sbert bug with filename.pt\nLoaded SBERT model for bug_with_filename\nLoading SBERT model from /kaggle/input/sbert-similarity/pytorch/default/1/sbert augmented bug without filename.pt\nLoaded SBERT model for bug_without_filename\nLoading SBERT model from /kaggle/input/sbert-similarity/pytorch/default/1/sbert augmented feature with filename.pt\nLoaded SBERT model for feature_with_filename\nLoading SBERT model from /kaggle/input/sbert-similarity/pytorch/default/1/sbert augmented features without filename.pt\nLoaded SBERT model for feature_without_filename\n\n==================================================\nPROCESSING BUG DATASET\n==================================================\n\nProcessing bug dataset: /kaggle/input/bug-and-feature-reference-data/bug reference data.csv\nLoaded 976 rows\nPreprocessing texts...\n","output_type":"stream"},{"name":"stderr","text":"Preprocessing (without filename): 100%|██████████| 976/976 [01:30<00:00, 10.73it/s]\nPreprocessing (with filename): 100%|██████████| 976/976 [00:23<00:00, 41.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Generating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"Generating embeddings (bug_without_filename): 100%|██████████| 31/31 [00:13<00:00,  2.28it/s]\nGenerating embeddings (bug_with_filename): 100%|██████████| 31/31 [00:13<00:00,  2.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Adding embedding columns to dataframe...\nSuccessfully processed bug dataset\nBug dataset saved to: /kaggle/working/bug reference data_with_embeddings.csv\nBug dataset processing time: 143.88 seconds\n\n==================================================\nPROCESSING FEATURE DATASET\n==================================================\n\nProcessing feature dataset: /kaggle/input/bug-and-feature-reference-data/feature reference data.csv\nLoaded 165 rows\nPreprocessing texts...\n","output_type":"stream"},{"name":"stderr","text":"Preprocessing (without filename): 100%|██████████| 165/165 [00:15<00:00, 10.59it/s]\nPreprocessing (with filename): 100%|██████████| 165/165 [00:02<00:00, 58.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Generating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"Generating embeddings (feature_without_filename): 100%|██████████| 6/6 [00:02<00:00,  2.58it/s]\nGenerating embeddings (feature_with_filename): 100%|██████████| 6/6 [00:02<00:00,  2.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Adding embedding columns to dataframe...\nSuccessfully processed feature dataset\nFeature dataset saved to: /kaggle/working/feature reference data_with_embeddings.csv\nFeature dataset processing time: 23.42 seconds\n\n==================================================\nPROCESSING COMPLETE\n==================================================\nBug dataset: 976 rows processed → /kaggle/working/bug reference data_with_embeddings.csv\nFeature dataset: 165 rows processed → /kaggle/working/feature reference data_with_embeddings.csv\nTotal processing time: 167.30 seconds\n\nEmbedding columns added:\n- 'without_filename_embeddings': Embeddings using without-filename preprocessing\n- 'with_filename_embeddings': Embeddings using with-filename preprocessing\n","output_type":"stream"}],"execution_count":13}]}